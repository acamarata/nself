from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from datetime import datetime
import os
from typing import Optional, List, Dict, Any

app = FastAPI(title="{{SERVICE_NAME}} - AI Agent Service")

# Request/Response models
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = "default"
    context: Optional[Dict[str, Any]] = {}

class ChatResponse(BaseModel):
    response: str
    session_id: str
    timestamp: str
    tools_used: Optional[List[str]] = []

# Simple in-memory session storage (use Redis in production)
sessions = {}

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "service": "{{SERVICE_NAME}}",
        "timestamp": datetime.now().isoformat(),
        "type": "ai-agent"
    }

@app.get("/api/info")
async def info():
    return {
        "service": "{{SERVICE_NAME}}",
        "project": "{{PROJECT_NAME}}",
        "framework": "LangChain + FastAPI",
        "capabilities": ["conversation", "tool-use", "memory"],
        "domain": "{{BASE_DOMAIN}}"
    }

@app.post("/chat")
async def chat(request: ChatRequest):
    """Main chat endpoint for AI agent interaction"""
    try:
        # Get or create session
        session = sessions.get(request.session_id, {
            "history": [],
            "context": request.context
        })
        
        # Simple echo response for template (replace with actual LangChain agent)
        # In production, you would:
        # 1. Initialize your LLM (OpenAI, Anthropic, Ollama, etc.)
        # 2. Create agent with tools
        # 3. Process the message through the agent
        # 4. Return the agent's response
        
        response_text = f"Echo from {{SERVICE_NAME}}: {request.message}"
        
        # Update session history
        session["history"].append({
            "user": request.message,
            "assistant": response_text,
            "timestamp": datetime.now().isoformat()
        })
        sessions[request.session_id] = session
        
        return ChatResponse(
            response=response_text,
            session_id=request.session_id,
            timestamp=datetime.now().isoformat(),
            tools_used=[]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/sessions/{session_id}")
async def get_session(session_id: str):
    """Get session history"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    return sessions[session_id]

@app.delete("/sessions/{session_id}")
async def clear_session(session_id: str):
    """Clear session history"""
    if session_id in sessions:
        del sessions[session_id]
    return {"message": "Session cleared"}

@app.get("/")
async def root():
    return {
        "message": "AI Agent Service for {{PROJECT_NAME}}",
        "service": "{{SERVICE_NAME}}",
        "endpoints": {
            "/chat": "POST - Send message to AI agent",
            "/sessions/{id}": "GET - Retrieve session history",
            "/health": "GET - Service health check",
            "/docs": "GET - Interactive API documentation"
        },
        "instructions": "Install langchain and your preferred LLM provider to enable AI features"
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 3000))
    print(f"ü§ñ {{SERVICE_NAME}} AI Agent is running on http://localhost:{port}")
    print(f"üìç Health check: http://localhost:{port}/health")
    print(f"üìö API docs: http://localhost:{port}/docs")
    print(f"üí¨ Chat endpoint: POST http://localhost:{port}/chat")
    uvicorn.run(app, host="0.0.0.0", port=port)