import os
from datetime import datetime
from typing import Any, Dict, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain.agents import initialize_agent, AgentType, Tool
from langchain.memory import ConversationBufferMemory
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import uvicorn

# Initialize FastAPI
app = FastAPI(
    title="{{SERVICE_NAME}}",
    description="LLM Agent service for {{PROJECT_NAME}}",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class QueryRequest(BaseModel):
    query: str
    context: Optional[Dict[str, Any]] = {}
    session_id: Optional[str] = None

class QueryResponse(BaseModel):
    response: str
    session_id: str
    timestamp: str

class HealthResponse(BaseModel):
    status: str
    service: str
    timestamp: str
    llm_configured: bool

# Memory storage (in production, use Redis or database)
memory_store = {}

# Initialize LLM (configure with your API key)
def get_llm():
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        return ChatOpenAI(
            api_key=api_key,
            model="gpt-3.5-turbo",
            temperature=0.7
        )
    # Fallback to mock LLM for demo
    return None

# Custom tools for the agent
def get_current_time(query: str) -> str:
    """Get the current time"""
    return datetime.utcnow().isoformat()

def calculate(query: str) -> str:
    """Perform simple calculations"""
    try:
        # Simple eval for demo - use proper parser in production
        result = eval(query, {"__builtins__": {}}, {})
        return str(result)
    except:
        return "Unable to calculate"

# Initialize tools
tools = [
    Tool(
        name="Current Time",
        func=get_current_time,
        description="Get the current UTC time"
    ),
    Tool(
        name="Calculator",
        func=calculate,
        description="Perform simple mathematical calculations"
    ),
]

# Initialize agent
def get_agent(session_id: str):
    llm = get_llm()
    if not llm:
        return None
    
    # Get or create memory for session
    if session_id not in memory_store:
        memory_store[session_id] = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
    
    memory = memory_store[session_id]
    
    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
        memory=memory,
        verbose=True
    )
    
    return agent

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    llm_configured = os.getenv("OPENAI_API_KEY") is not None
    return HealthResponse(
        status="healthy",
        service="{{SERVICE_NAME}}",
        timestamp=datetime.utcnow().isoformat(),
        llm_configured=llm_configured
    )

@app.get("/api/info")
async def get_info():
    """Service information endpoint"""
    return {
        "service": "{{SERVICE_NAME}}",
        "project": "{{PROJECT_NAME}}",
        "framework": "LangChain",
        "runtime": "Python",
        "domain": "{{BASE_DOMAIN}}",
        "capabilities": {
            "llm": "OpenAI GPT-3.5/4",
            "agents": True,
            "memory": True,
            "tools": [tool.name for tool in tools]
        }
    }

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Hello from {{SERVICE_NAME}}!",
        "project": "{{PROJECT_NAME}}",
        "framework": "LangChain - Building applications with LLMs",
        "endpoints": {
            "health": "/health",
            "info": "/api/info",
            "query": "/api/query"
        }
    }

@app.post("/api/query", response_model=QueryResponse)
async def query_agent(request: QueryRequest):
    """Query the LLM agent"""
    session_id = request.session_id or f"session_{datetime.utcnow().timestamp()}"
    
    agent = get_agent(session_id)
    if not agent:
        # Fallback response when LLM not configured
        response = f"Echo: {request.query} (LLM not configured - set OPENAI_API_KEY)"
    else:
        try:
            # Run agent with the query
            response = agent.run(request.query)
        except Exception as e:
            response = f"Error processing query: {str(e)}"
    
    return QueryResponse(
        response=response,
        session_id=session_id,
        timestamp=datetime.utcnow().isoformat()
    )

@app.post("/api/reset/{session_id}")
async def reset_session(session_id: str):
    """Reset conversation memory for a session"""
    if session_id in memory_store:
        del memory_store[session_id]
        return {"status": "success", "message": f"Session {session_id} reset"}
    return {"status": "not_found", "message": f"Session {session_id} not found"}

@app.get("/api/sessions")
async def list_sessions():
    """List active sessions"""
    return {
        "sessions": list(memory_store.keys()),
        "count": len(memory_store)
    }

if __name__ == "__main__":
    port = int(os.getenv("PORT", 3000))
    
    print(f"üöÄ {{SERVICE_NAME}} is starting on port {port}")
    print(f"üìç Health check: http://localhost:{port}/health")
    print(f"üåê API endpoint: http://localhost:{port}/api/info")
    print(f"ü§ñ Query endpoint: POST http://localhost:{port}/api/query")
    print(f"üí° Set OPENAI_API_KEY environment variable to enable LLM")
    
    uvicorn.run(app, host="0.0.0.0", port=port)