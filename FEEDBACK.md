# nself CLI - Database & Deployment Automation FEEDBACK

**Date:** February 11, 2026, 11:00 PM EST
**Project:** nself-web (Production Dogfooding)
**Context:** Building out BASE app + PING service with complete database/seeding/deployment workflow
**Status:** ğŸŸ¡ **CRITICAL AUTOMATION GAPS IDENTIFIED**

---

## ğŸ¯ EXECUTIVE SUMMARY

While working on the complete implementation of the BASE admin dashboard and PING telemetry service, we discovered **critical gaps in nself CLI automation** for database management and deployment workflows.

**What Works:**
- âœ… Docker container orchestration (9/9 services healthy)
- âœ… Environment variable management (`.environments/` structure)
- âœ… Service composition (core + optional + custom services)
- âœ… Init containers for permissions

**What's Missing:**
- âŒ Database migration automation (`hasura migrations apply`)
- âŒ Seed data automation (`hasura seeds apply`)
- âŒ Environment-specific seeding (dev/staging/prod data strategies)
- âŒ Hasura metadata management (GraphQL schema, permissions)
- âŒ Custom service database integration helpers
- âŒ Complete deployment workflow (DB + app deployment together)

---

## ğŸ“Š CURRENT STATE ASSESSMENT

### Infrastructure Created (Manual Process)

We manually built a complete production-ready database schema:

**Database Schema:**
- 39 tables across 9 migrations
- Users, roles, permissions (RBAC system)
- Licenses and telemetry (for nself CLI tracking)
- Organizations and cloud servers
- Chat and content management
- Billing and notifications
- Subscription tiers and verification

**Seed Data:**
- System roles: owner, admin, support, user, guest
- Production owner: alisalaah@gmail.com
- Dev/staging staff: owner@nself.org, admin@nself.org, support@nself.org
- Demo users: alice/bob/carol/dave@demo.com
- Demo licenses, organizations, telemetry data

**Custom Services:**
- PING API (telemetry ingestion) - currently in-memory, needs DB integration

### Manual Steps Required (Should Be Automated)

```bash
# Step 1: Apply Hasura migrations (NO nself COMMAND EXISTS)
cd ~/Sites/nself-web/backend/hasura/migrations/default
for dir in */; do
  cat "$dir/up.sql" | docker exec -i nself-web_postgres psql -U postgres -d nself_web_db
done

# Step 2: Apply seed data (NO nself COMMAND EXISTS)
cd ~/Sites/nself-web/backend/hasura/seeds/default
for seed in *.sql; do
  cat "$seed" | docker exec -i nself-web_postgres psql -U postgres -d nself_web_db
done

# Step 3: Apply Hasura metadata (NO nself COMMAND EXISTS)
# Would need hasura CLI or manual GraphQL mutations

# Step 4: Configure custom service DB connection (MANUAL)
# Edit ping_api/src/index.ts to add Postgres client
# No helper to auto-inject DATABASE_URL

# Step 5: Deploy to staging/prod (PARTIALLY AUTOMATED)
nself deploy staging  # Exists but doesn't handle DB migrations/seeds
```

---

## ğŸš¨ CRITICAL GAPS IN nself CLI

### Gap #1: Database Migration Management

**Current State:** âŒ No automation

**What's Needed:**
```bash
# Proposed commands
nself migrate init              # Initialize migrations directory
nself migrate create <name>     # Create new migration
nself migrate up                # Apply pending migrations
nself migrate down              # Rollback last migration
nself migrate status            # Show migration status
nself migrate reset             # Reset database (dev only)
```

**How It Should Work:**
1. Detect `hasura/migrations/` directory exists
2. Connect to Postgres container (use DATABASE_URL from .env)
3. Apply migrations in order
4. Track applied migrations in `schema_migrations` table
5. Support rollback with `down.sql` files

**Priority:** ğŸ”´ **CRITICAL** - Without this, every developer must manually run SQL scripts

**Suggested Implementation:**
- Use Hasura CLI under the hood (if installed)
- Or implement simple migration runner in bash:
  ```bash
  # src/lib/database/migrate.sh
  migrate_up() {
    local migrations_dir="hasura/migrations/default"
    for migration in $(ls -1 "$migrations_dir" | sort); do
      local up_sql="$migrations_dir/$migration/up.sql"
      if [ -f "$up_sql" ]; then
        echo "Applying: $migration"
        docker exec -i "${PROJECT_NAME}_postgres" \
          psql -U postgres -d "${POSTGRES_DB}" < "$up_sql"
      fi
    done
  }
  ```

---

### Gap #2: Seed Data Management

**Current State:** âŒ No automation

**What's Needed:**
```bash
# Proposed commands
nself seed                      # Apply all seeds for current ENV
nself seed --env=dev            # Apply dev seeds
nself seed --env=staging        # Apply staging seeds
nself seed --env=prod           # Apply production seeds only
nself seed --file=<path>        # Apply specific seed file
nself seed reset                # Clear and re-seed (dev only)
```

**Environment-Specific Seed Strategy:**

**Production:**
- `000_production_owner.sql` (alisalaah@gmail.com)
- `001_system_roles_and_permissions.sql` (RBAC setup)
- NO demo data

**Staging:**
- All production seeds
- `002_demo_users.sql` (staff test accounts)
- `003_demo_licenses.sql` (test licenses)
- `004_demo_organizations_and_servers.sql` (test orgs)
- Limited demo data for testing

**Development:**
- All production + staging seeds
- `005_demo_chat_workspace.sql` (full chat demo)
- `006_demo_telemetry_and_stats.sql` (analytics demo)
- `007_subscription_and_verification_seeds.sql` (subscription demo)
- Extensive demo data for UI development

**How It Should Work:**
1. Detect `hasura/seeds/` directory exists
2. Filter seeds by environment (naming convention or metadata)
3. Apply seeds in alphabetical order (001_, 002_, etc.)
4. Skip seeds that shouldn't run in current ENV
5. Handle `ON CONFLICT` clauses properly (idempotent)

**Priority:** ğŸ”´ **CRITICAL** - Teams waste hours manually seeding databases

**Suggested Implementation:**
```bash
# src/lib/database/seed.sh
seed_database() {
  local env="${ENV:-dev}"
  local seeds_dir="hasura/seeds/default"

  # Define which seeds run in which environment
  case "$env" in
    prod)
      # Production: Only critical system seeds
      local allowed_seeds="000_production_owner 001_system_roles"
      ;;
    staging)
      # Staging: System + some demo data
      local allowed_seeds="000_production_owner 001_system_roles 002_demo_users 003_demo_licenses"
      ;;
    dev)
      # Dev: Everything
      local allowed_seeds="*"
      ;;
  esac

  for seed in $(ls -1 "$seeds_dir"/*.sql | sort); do
    local seed_name=$(basename "$seed" .sql)

    # Check if seed is allowed in this environment
    if should_apply_seed "$seed_name" "$allowed_seeds"; then
      echo "Applying seed: $seed_name"
      docker exec -i "${PROJECT_NAME}_postgres" \
        psql -U postgres -d "${POSTGRES_DB}" < "$seed"
    else
      echo "Skipping seed: $seed_name (not for $env)"
    fi
  done
}
```

---

### Gap #3: Hasura Metadata Management

**Current State:** âŒ No automation

**What's Needed:**
```bash
# Proposed commands
nself hasura metadata export    # Export current metadata to files
nself hasura metadata apply     # Apply metadata from files
nself hasura metadata diff      # Show metadata diff
nself hasura metadata reload    # Reload Hasura metadata
nself hasura console            # Open Hasura console (local)
```

**What Metadata Includes:**
- GraphQL schema and relationships
- Hasura permissions (role-based access)
- Actions, events, remote schemas
- REST endpoints
- Custom functions

**How It Should Work:**
1. Use Hasura GraphQL API (metadata management endpoint)
2. Read `hasura/metadata/` directory
3. POST to `http://hasura:8080/v1/metadata` with admin secret
4. Apply tables, relationships, permissions defined in YAML

**Priority:** ğŸŸ¡ **HIGH** - GraphQL doesn't work without metadata

**Suggested Implementation:**
```bash
# src/lib/hasura/metadata.sh
apply_hasura_metadata() {
  local hasura_url="http://localhost:${HASURA_PORT:-8080}"
  local admin_secret="${HASURA_GRAPHQL_ADMIN_SECRET}"
  local metadata_dir="hasura/metadata"

  # Apply metadata via Hasura API
  curl -X POST \
    -H "X-Hasura-Admin-Secret: $admin_secret" \
    -H "Content-Type: application/json" \
    -d @"$metadata_dir/metadata.json" \
    "$hasura_url/v1/metadata"
}
```

---

### Gap #4: Custom Service Database Integration

**Current State:** âŒ Manual configuration required

**What's Needed:**

When a custom service needs database access (like PING API), developers must:
1. Manually add Postgres client library to `package.json`
2. Manually configure connection string
3. Manually write database queries
4. Manually handle connection pooling

**Proposed Solution:**

```bash
# New command
nself service db enable ping_api

# What it does:
# 1. Adds pg library to package.json
# 2. Injects DATABASE_URL env var automatically
# 3. Creates src/lib/db.ts with connection pool
# 4. Adds example queries in src/lib/queries.ts
```

**Generated `src/lib/db.ts`:**
```typescript
import { Pool } from 'pg';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

export const query = (text: string, params?: any[]) => pool.query(text, params);
export default pool;
```

**Priority:** ğŸŸ¡ **MEDIUM** - Helps developers integrate DB faster

---

### Gap #5: Integrated Deployment Workflow

**Current State:** âš ï¸ **PARTIAL** - `nself deploy` exists but incomplete

**What Exists:**
```bash
nself deploy prod              # Deploys backend services
nself deploy staging           # Deploys full stack
nself deploy health prod       # Health check
```

**What's Missing:**
- âŒ Database migration during deployment
- âŒ Seed data during first-time deployment
- âŒ Hasura metadata sync
- âŒ Pre-deployment database backup
- âŒ Post-deployment smoke tests

**Proposed Complete Workflow:**

```bash
nself deploy staging --with-db

# Deployment steps:
# 1. âœ“ SSH into staging server
# 2. âœ“ Pull latest code
# 3. âœ“ Build docker-compose.yml
# 4. âœ— Backup database (MISSING)
# 5. âœ— Apply pending migrations (MISSING)
# 6. âœ— Apply environment-specific seeds (MISSING)
# 7. âœ— Apply Hasura metadata (MISSING)
# 8. âœ“ Restart Docker services
# 9. âœ— Run smoke tests (MISSING)
# 10. âœ“ Health check
```

**Priority:** ğŸ”´ **CRITICAL** - Deployments currently error-prone

**Suggested Deployment Flow:**
```bash
# src/lib/deploy/deploy-with-db.sh
deploy_with_database() {
  local env="$1"
  local server_config=".environments/$env/server.json"

  # Read server config
  local ssh_host=$(jq -r '.ssh.host' "$server_config")
  local ssh_user=$(jq -r '.ssh.user' "$server_config")
  local remote_path=$(jq -r '.deployment.remote_path' "$server_config")

  # SSH and run deployment
  ssh "${ssh_user}@${ssh_host}" << 'ENDSSH'
    cd "${remote_path}/backend"

    # Backup database
    docker exec "${PROJECT_NAME}_postgres" \
      pg_dump -U postgres "${POSTGRES_DB}" > backup_$(date +%Y%m%d_%H%M%S).sql

    # Apply migrations
    nself migrate up

    # Apply seeds (environment-aware)
    nself seed --env="${ENV}"

    # Apply Hasura metadata
    nself hasura metadata apply

    # Restart services
    nself restart

    # Smoke tests
    nself test smoke
ENDSSH
}
```

---

## ğŸ¯ RECOMMENDED IMPLEMENTATION PRIORITY

### Phase 1: Database Essentials (v0.10.0)
**Timeline:** 2-3 weeks
**Impact:** ğŸ”´ Critical - Blocks production readiness

1. **`nself migrate` commands** (highest priority)
   - `nself migrate up` - Apply pending migrations
   - `nself migrate status` - Show migration state
   - `nself migrate create <name>` - Create new migration

2. **`nself seed` commands**
   - `nself seed` - Apply environment-aware seeds
   - `nself seed --env=<env>` - Override environment
   - `nself seed reset` - Clear and re-seed (dev only)

3. **Environment-specific seed filtering**
   - Read seed metadata or use naming conventions
   - Apply only appropriate seeds per environment

### Phase 2: Hasura Integration (v0.10.1)
**Timeline:** 1-2 weeks
**Impact:** ğŸŸ¡ High - Required for GraphQL

1. **`nself hasura` commands**
   - `nself hasura metadata apply` - Apply metadata from files
   - `nself hasura metadata export` - Export to files
   - `nself hasura console` - Open Hasura console

2. **Automatic metadata sync on deploy**

### Phase 3: Deployment Enhancement (v0.10.2)
**Timeline:** 2 weeks
**Impact:** ğŸŸ¡ High - Improves deployment safety

1. **Database backup before deployment**
2. **Automatic migration during deployment**
3. **Post-deployment smoke tests**
4. **Rollback capability**

### Phase 4: Developer Experience (v0.11.0)
**Timeline:** 1 week
**Impact:** ğŸŸ¢ Medium - Nice to have

1. **`nself service db enable <name>`** - Auto-configure DB for custom services
2. **`nself db shell`** - Open psql shell in container
3. **`nself db backup`** - Create database backup
4. **`nself db restore <file>`** - Restore from backup

---

## ğŸ“‹ DETAILED COMMAND SPECIFICATIONS

### `nself migrate up`

**Description:** Apply all pending database migrations

**Usage:**
```bash
nself migrate up                # Apply all pending
nself migrate up --steps=1      # Apply next 1 migration
nself migrate up --to=<version> # Apply up to specific version
```

**Behavior:**
1. Check for `hasura/migrations/default/` directory
2. Read `schema_migrations` table for applied migrations
3. Apply migrations in version order (timestamp-based)
4. Update `schema_migrations` table
5. Show success/failure for each migration

**Example Output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ nself migrate up                                          â•‘
â•‘ Apply pending database migrations                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Checking for pending migrations...
âœ“ Found 3 pending migrations

â†’ Applying migrations:
  âœ“ 1706140800000_initial_enums_and_users (0.5s)
  âœ“ 1706140801000_roles_and_permissions (0.3s)
  âœ“ 1706140802000_licenses_and_telemetry (0.4s)

âœ“ All migrations applied successfully
âœ“ Database is up to date

Run 'nself seed' to populate data
```

**Error Handling:**
- If migration fails, stop and show SQL error
- Don't mark failed migration as applied
- Suggest rollback or manual fix

---

### `nself migrate status`

**Description:** Show migration status

**Usage:**
```bash
nself migrate status
```

**Example Output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ nself migrate status                                      â•‘
â•‘ Database migration status                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Migration Status:

  âœ“ 1706140800000_initial_enums_and_users      (applied 2 days ago)
  âœ“ 1706140801000_roles_and_permissions        (applied 2 days ago)
  âœ“ 1706140802000_licenses_and_telemetry       (applied 2 days ago)
  â—‹ 1706140809000_new_feature                  (pending)
  â—‹ 1706140810000_another_feature              (pending)

âœ“ 3 migrations applied
â³ 2 migrations pending

Run 'nself migrate up' to apply pending migrations
```

---

### `nself seed`

**Description:** Apply environment-specific seed data

**Usage:**
```bash
nself seed                      # Apply seeds for current ENV
nself seed --env=dev            # Override environment
nself seed --file=001_roles.sql # Apply specific seed
nself seed --reset              # Clear and re-seed (dev only)
```

**Behavior:**
1. Detect current environment from `ENV` variable
2. Filter seeds based on environment rules
3. Apply seeds in alphabetical order
4. Skip already-seeded data (idempotent via `ON CONFLICT`)

**Example Output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ nself seed                                                â•‘
â•‘ Apply environment-specific seed data                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Environment: dev
â†’ Seed Strategy: Apply all seeds (development mode)

â†’ Applying seeds:
  âœ“ 000_production_owner.sql              (1 row)
  âœ“ 001_system_roles_and_permissions.sql  (35 rows)
  âœ“ 002_demo_users.sql                    (7 users)
  âœ“ 003_demo_licenses.sql                 (3 licenses)
  âœ“ 004_demo_organizations_and_servers.sql (5 orgs, 12 servers)
  â—‹ 005_demo_chat_workspace.sql           (skipped - errors in seed)
  âœ“ 006_demo_telemetry_and_stats.sql      (50 events)
  âœ“ 007_subscription_and_verification_seeds.sql (20 rows)

âœ“ Seeding complete
âœ“ Database ready for development

Login with: owner@nself.org / npass123
```

**Environment Rules:**
- **Production:** Only `000_*` and `001_*` (system essentials)
- **Staging:** `000_*` through `004_*` (basic demo data)
- **Development:** All seeds (full demo environment)

---

### `nself hasura metadata apply`

**Description:** Apply Hasura metadata (GraphQL schema, permissions)

**Usage:**
```bash
nself hasura metadata apply
nself hasura metadata apply --force  # Override conflicts
```

**Behavior:**
1. Read `hasura/metadata/` directory
2. Parse YAML files (tables, relationships, permissions)
3. POST to Hasura metadata API
4. Reload Hasura schema

**Example Output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ nself hasura metadata apply                               â•‘
â•‘ Apply GraphQL metadata and permissions                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Reading metadata from: hasura/metadata/

  âœ“ tables.yaml          (39 tables, 127 relationships)
  âœ“ actions.yaml         (5 actions)
  âœ“ permissions.yaml     (89 role permissions)

â†’ Applying to Hasura:
  âœ“ Metadata applied successfully
  âœ“ Schema reloaded

âœ“ GraphQL endpoint ready at: https://api.local.nself.org/v1/graphql
```

---

## ğŸ’¡ ARCHITECTURAL RECOMMENDATIONS

### Recommendation #1: Migration File Structure

**Current:** Hasura-style migrations (good!)
```
hasura/
â”œâ”€â”€ migrations/
â”‚   â””â”€â”€ default/
â”‚       â”œâ”€â”€ 1706140800000_initial/
â”‚       â”‚   â”œâ”€â”€ up.sql
â”‚       â”‚   â””â”€â”€ down.sql
â”‚       â””â”€â”€ 1706140801000_roles/
â”‚           â”œâ”€â”€ up.sql
â”‚           â””â”€â”€ down.sql
â””â”€â”€ seeds/
    â””â”€â”€ default/
        â”œâ”€â”€ 000_production_owner.sql
        â”œâ”€â”€ 001_system_roles.sql
        â””â”€â”€ 002_demo_users.sql
```

**Keep this structure!** It's compatible with Hasura CLI and well-organized.

### Recommendation #2: Environment-Aware Seed Metadata

**Option A: Naming Convention (Recommended - Simple)**
```
000_production_owner.sql       # prod, staging, dev
001_system_roles.sql           # prod, staging, dev
002_demo_users.sql             # staging, dev only
003_demo_licenses.sql          # staging, dev only
005_demo_chat.sql              # dev only
```

Rules:
- `000-001`: Always run (production-safe)
- `002-004`: Staging + dev only
- `005+`: Dev only

**Option B: Metadata Files (More Flexible)**
```
seeds/
â”œâ”€â”€ default/
â”‚   â”œâ”€â”€ 001_system_roles.sql
â”‚   â”œâ”€â”€ 001_system_roles.meta.json
â”‚   â”œâ”€â”€ 002_demo_users.sql
â”‚   â””â”€â”€ 002_demo_users.meta.json
```

`002_demo_users.meta.json`:
```json
{
  "environments": ["dev", "staging"],
  "description": "Demo user accounts for testing",
  "idempotent": true
}
```

### Recommendation #3: Database Connection Helpers

For custom services like PING API, auto-inject database utilities:

**Auto-generated `src/lib/db.ts`:**
```typescript
import { Pool, QueryResult } from 'pg';

// Auto-configured from DATABASE_URL environment variable
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: parseInt(process.env.DB_POOL_SIZE || '20', 10),
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

// Helper for simple queries
export async function query<T = any>(
  text: string,
  params?: any[]
): Promise<QueryResult<T>> {
  const start = Date.now();
  const res = await pool.query<T>(text, params);
  const duration = Date.now() - start;

  if (process.env.DEBUG_SQL === 'true') {
    console.log('Executed query', { text, duration, rows: res.rowCount });
  }

  return res;
}

// Transaction helper
export async function transaction<T>(
  callback: (client: any) => Promise<T>
): Promise<T> {
  const client = await pool.connect();
  try {
    await client.query('BEGIN');
    const result = await callback(client);
    await client.query('COMMIT');
    return result;
  } catch (error) {
    await client.query('ROLLBACK');
    throw error;
  } finally {
    client.release();
  }
}

export default pool;
```

---

## ğŸ§ª TESTING REQUIREMENTS

### Unit Tests for New Commands

```bash
# Test migration apply
test_migrate_up_applies_pending_migrations() {
  # Setup: Fresh database
  nself clean
  nself start

  # Execute: Apply migrations
  nself migrate up

  # Verify: Tables exist
  local table_count=$(psql_query "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='public'")
  assert_equals "$table_count" "39"
}

# Test environment-specific seeding
test_seed_production_only_applies_safe_seeds() {
  # Setup: Production environment
  export ENV=prod
  nself migrate up

  # Execute: Seed database
  nself seed

  # Verify: Only production owner exists, no demo users
  local user_count=$(psql_query "SELECT COUNT(*) FROM users")
  assert_equals "$user_count" "1"  # Only production owner

  local demo_user=$(psql_query "SELECT COUNT(*) FROM users WHERE primary_email='alice@demo.com'")
  assert_equals "$demo_user" "0"  # No demo users in prod
}
```

### Integration Tests

```bash
# Test complete deployment workflow
test_deploy_with_database() {
  # Setup: Staging environment
  export ENV=staging

  # Execute: Deploy with database
  nself deploy staging --with-db

  # Verify: Services running
  nself status | grep "9/9 healthy"

  # Verify: Database migrated
  ssh staging "docker exec postgres psql -c 'SELECT COUNT(*) FROM schema_migrations'"

  # Verify: Seeds applied
  ssh staging "docker exec postgres psql -c 'SELECT COUNT(*) FROM users WHERE role=\\'owner\\\''"
}
```

---

## ğŸ“š DOCUMENTATION REQUIREMENTS

### New Docs Pages Needed

1. **Database Management Guide**
   - URL: `docs.nself.org/database/overview`
   - Content: How to create migrations, seed data, manage schema

2. **Deployment with Database Guide**
   - URL: `docs.nself.org/deployment/with-database`
   - Content: End-to-end deployment including DB sync

3. **Custom Service Database Integration**
   - URL: `docs.nself.org/services/database-integration`
   - Content: How to add Postgres to custom services

4. **Environment-Specific Seeding**
   - URL: `docs.nself.org/database/seeding-strategies`
   - Content: How to structure seeds for dev/staging/prod

### Updated Docs Pages

1. **Quick Start** - Add database setup step
2. **CLI Reference** - Add new commands
3. **Deployment** - Include database migration steps

---

## ğŸ“ REAL-WORLD USAGE EXAMPLE

### Scenario: New nself-web Team Member Onboarding

**Current Experience (Without Automation):**
```bash
# Day 1: Setup takes 4-6 hours with manual DB work
git clone https://github.com/acamarata/nself-web.git
cd nself-web/backend

# Start services
nself build && nself start

# Manually run migrations (30+ minutes of confusion)
# - Find migration files
# - Figure out how to run them
# - Debug connection issues
# - Apply in correct order

for dir in hasura/migrations/default/*/; do
  cat "$dir/up.sql" | docker exec -i nself-web_postgres psql -U postgres -d nself_web_db
done

# Manually run seeds (15+ minutes)
for seed in hasura/seeds/default/*.sql; do
  cat "$seed" | docker exec -i nself-web_postgres psql -U postgres -d nself_web_db
done

# Manually configure PING service DB connection (30+ minutes)
# - Add pg library to package.json
# - Write connection code
# - Test queries

# Finally: Start frontend
npm run dev

# Total time: 4-6 hours (frustrating experience)
```

**Desired Experience (With Automation):**
```bash
# Day 1: Setup takes 10 minutes
git clone https://github.com/acamarata/nself-web.git
cd nself-web/backend

# One command does everything
nself dev

# Behind the scenes:
# 1. Builds docker-compose.yml
# 2. Starts all services
# 3. Applies database migrations
# 4. Seeds database with dev data
# 5. Applies Hasura metadata
# 6. Configures service DB connections
# 7. Runs health checks

# Output:
# âœ“ All services started successfully (9/9 healthy)
# âœ“ Database migrated (39 tables created)
# âœ“ Database seeded with demo data
# âœ“ GraphQL endpoint ready at: https://api.local.nself.org/v1/graphql
# âœ“ Login with: owner@nself.org / npass123

# Start frontend
npm run dev

# Total time: 10 minutes (amazing experience!)
```

---

## ğŸš€ IMPACT ASSESSMENT

### Current Pain Points

**For nself-web Team:**
- â±ï¸ **4-6 hours** wasted per developer on initial setup
- ğŸ˜¤ **Frustration** from manual SQL scripts
- ğŸ› **Bugs** from inconsistent database state
- âš ï¸ **Risk** of deploying without migrations

**For Future nself Users:**
- ğŸš« **Blocked** from using Hasura (no metadata automation)
- ğŸ“š **Steep learning curve** (must learn Hasura CLI separately)
- ğŸ”„ **Inconsistent** dev/staging/prod databases
- â¸ï¸ **Slow adoption** due to complexity

### Expected Impact After Automation

**Time Savings:**
- Setup time: **4-6 hours â†’ 10 minutes** (96% reduction)
- Deployment time: **30-60 minutes â†’ 5 minutes** (90% reduction)
- Debugging database issues: **Hours â†’ Minutes**

**Quality Improvements:**
- âœ… Consistent database state across all environments
- âœ… No more "it works on my machine" (DB version)
- âœ… Automated testing of database changes
- âœ… Safe deployments with automatic backups

**Adoption:**
- ğŸ“ˆ **10x easier** to get started with nself
- ğŸ¯ **Production-ready** database workflows
- ğŸ’ª **Confidence** in deployment process
- ğŸŒŸ **Best-in-class** DX for local-first infrastructure

---

## ğŸ¯ SUCCESS CRITERIA

nself CLI database automation will be considered **complete** when:

1. âœ… A new developer can run `nself dev` and have a fully seeded database in &lt;2 minutes
2. âœ… `nself deploy staging --with-db` automatically migrates and seeds the database
3. âœ… Production deployments include automatic pre-deployment backup
4. âœ… No manual SQL execution required for normal development workflow
5. âœ… Custom services can enable database access with `nself service db enable <name>`
6. âœ… All database operations are environment-aware (dev/staging/prod)
7. âœ… Hasura GraphQL works out-of-box after `nself dev`
8. âœ… Documentation covers all database workflows
9. âœ… Integration tests validate migration + seeding + deployment
10. âœ… The nself-web team (dogfooding) uses these commands exclusively

---

## ğŸ’¬ ADDITIONAL CONTEXT

### Why This Matters

nself CLI aims to be the **best local-first infrastructure tool** for building production-ready backends. Database management is **50% of backend work**, yet it's currently 100% manual.

Without database automation, nself CLI is:
- âš ï¸ **Incomplete** for production use
- ğŸ˜¤ **Frustrating** for new users
- ğŸ› **Error-prone** (manual SQL)
- â±ï¸ **Slow** to onboard

With database automation, nself CLI becomes:
- âœ… **Complete** end-to-end solution
- ğŸ‰ **Delightful** developer experience
- ğŸ›¡ï¸ **Safe** and reliable
- âš¡ **Fast** to get started

### Comparison to Competitors

**Supabase:**
- âœ… Has database migration CLI (`supabase db push`)
- âœ… Has seeding (`supabase db seed`)
- âœ… Automatic metadata sync

**Hasura:**
- âœ… Has migration CLI (`hasura migrate apply`)
- âœ… Has seeding (`hasura seed apply`)
- âœ… Has metadata management

**nself CLI (Current):**
- âŒ No migration automation
- âŒ No seeding automation
- âŒ No metadata management

**nself CLI needs parity with Supabase/Hasura to be competitive.**

---

## ğŸ“ NEXT STEPS

### For nself CLI Team

**Immediate (This Week):**
1. Review this feedback
2. Prioritize features (we recommend Phase 1 first)
3. Create GitHub issues for each command
4. Assign to sprint

**Short-Term (Next 2-3 Weeks):**
1. Implement `nself migrate` commands
2. Implement `nself seed` commands
3. Test with nself-web project
4. Document in CLI guide

**Medium-Term (Next Month):**
1. Implement `nself hasura` commands
2. Enhance `nself deploy` with database sync
3. Add developer experience improvements
4. Release as v0.10.0

### For nself-web Team

**Immediate:**
1. Continue manual workflow (documented in this feedback)
2. Provide testing/validation for new commands
3. Report bugs and edge cases

**After Automation:**
1. Migrate to automated workflow
2. Update team documentation
3. Reduce onboarding time from hours to minutes
4. Deploy to production with confidence

---

## ğŸ™ THANK YOU

This feedback comes from **real production usage** of nself CLI. We're committed to making nself the best infrastructure tool available, and that requires excellent database automation.

The nself-web team is excited to help test and validate these new features. We believe these improvements will:
- ğŸš€ **10x adoption** of nself CLI
- âš¡ **10x developer productivity**
- ğŸ›¡ï¸ **10x deployment safety**

We're ready to collaborate on design, testing, and documentation for these features.

---

**Contact:** nself-web Team
**Project:** BASE admin dashboard + PING telemetry service
**Current Status:** Fully functional with manual database workflow
**Desired Status:** Fully automated database management via nself CLI
**Priority:** ğŸ”´ **CRITICAL** - Blocks production deployment

---

*Feedback generated from real-world dogfooding of nself CLI*
*Date: February 11, 2026*
*Status: Ready for nself CLI team review and implementation*
