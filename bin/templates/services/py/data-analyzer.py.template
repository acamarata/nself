from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from datetime import datetime, timedelta
import os
import uvicorn
import asyncio
import logging
from typing import List, Dict, Any, Optional
import json
import redis
import httpx
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Data Analyzer",
    description="ML/AI Data Analysis Service for ${PROJECT_NAME}",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration
REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379")
HASURA_ENDPOINT = os.getenv("HASURA_ENDPOINT", "http://hasura:8080/v1/graphql")
HASURA_SECRET = os.getenv("HASURA_ADMIN_SECRET", "hasura-admin-secret")

# Redis connection
try:
    redis_client = redis.from_url(REDIS_URL)
    redis_client.ping()
    logger.info("âœ… Connected to Redis")
except Exception as e:
    logger.warning(f"âš ï¸ Redis connection failed: {e}")
    redis_client = None

# Pydantic models
class HealthResponse(BaseModel):
    status: str
    service: str
    timestamp: datetime
    redis_connected: bool
    ml_models_loaded: bool

class AnalysisRequest(BaseModel):
    data_type: str = Field(..., description="Type of data to analyze (weather, currency, crypto)")
    analysis_type: str = Field(..., description="Type of analysis (trend, prediction, correlation, anomaly)")
    timeframe: str = Field(default="7d", description="Timeframe for analysis (1h, 1d, 7d, 30d)")
    parameters: Dict[str, Any] = Field(default={}, description="Additional parameters")

class PredictionRequest(BaseModel):
    data_type: str
    symbol: str  # e.g., "BTC", "EUR", "New York"
    days_ahead: int = Field(default=7, ge=1, le=30)
    model_type: str = Field(default="linear", description="linear, polynomial, or moving_average")

class CorrelationRequest(BaseModel):
    data_type1: str
    symbol1: str
    data_type2: str
    symbol2: str
    timeframe: str = Field(default="30d")

class AnomalyDetectionRequest(BaseModel):
    data_type: str
    symbol: str
    sensitivity: float = Field(default=2.0, ge=1.0, le=5.0)
    window_size: int = Field(default=24, ge=6, le=168)

# Data Analysis Service
class DataAnalyzer:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.analysis_cache = {}
        
    async def get_time_series_data(self, data_type: str, symbol: str, timeframe: str) -> pd.DataFrame:
        """Fetch time series data from Hasura GraphQL"""
        
        # Calculate time range
        now = datetime.utcnow()
        if timeframe == "1h":
            since = now - timedelta(hours=1)
        elif timeframe == "1d":
            since = now - timedelta(days=1)
        elif timeframe == "7d":
            since = now - timedelta(days=7)
        elif timeframe == "30d":
            since = now - timedelta(days=30)
        else:
            since = now - timedelta(days=7)
        
        # Build GraphQL query based on data type
        if data_type == "weather":
            query = """
                query GetWeatherData($city: String!, $since: timestamptz!) {
                    weather_data(
                        where: { 
                            city: { _eq: $city }
                            created_at: { _gte: $since }
                        }
                        order_by: { created_at: asc }
                    ) {
                        temperature
                        humidity
                        pressure
                        created_at
                    }
                }
            """
            variables = {"city": symbol, "since": since.isoformat()}
            
        elif data_type == "crypto":
            query = """
                query GetCryptoData($symbol: String!, $since: timestamptz!) {
                    crypto_prices(
                        where: { 
                            symbol: { _eq: $symbol }
                            created_at: { _gte: $since }
                        }
                        order_by: { created_at: asc }
                    ) {
                        price
                        market_cap
                        volume_24h
                        change_24h
                        created_at
                    }
                }
            """
            variables = {"symbol": symbol.upper(), "since": since.isoformat()}
            
        elif data_type == "currency":
            query = """
                query GetCurrencyData($target: String!, $since: timestamptz!) {
                    exchange_rates(
                        where: { 
                            target_currency: { _eq: $target }
                            base_currency: { _eq: "USD" }
                            created_at: { _gte: $since }
                        }
                        order_by: { created_at: asc }
                    ) {
                        rate
                        created_at
                    }
                }
            """
            variables = {"target": symbol.upper(), "since": since.isoformat()}
            
        else:
            raise ValueError(f"Unsupported data type: {data_type}")
        
        # Execute GraphQL query
        async with httpx.AsyncClient() as client:
            response = await client.post(
                HASURA_ENDPOINT,
                json={"query": query, "variables": variables},
                headers={
                    "Content-Type": "application/json",
                    "X-Hasura-Admin-Secret": HASURA_SECRET,
                }
            )
            
            if response.status_code != 200:
                raise HTTPException(status_code=500, detail="Failed to fetch data from database")
            
            result = response.json()
            if "errors" in result:
                raise HTTPException(status_code=500, detail=f"GraphQL error: {result['errors']}")
            
            # Convert to DataFrame
            if data_type == "weather":
                data = result["data"]["weather_data"]
                if not data:
                    # Generate sample data for demo
                    data = self._generate_sample_weather_data(symbol, since, now)
                df = pd.DataFrame(data)
                df['created_at'] = pd.to_datetime(df['created_at'])
                df.set_index('created_at', inplace=True)
                
            elif data_type == "crypto":
                data = result["data"]["crypto_prices"]
                if not data:
                    # Generate sample data for demo
                    data = self._generate_sample_crypto_data(symbol, since, now)
                df = pd.DataFrame(data)
                df['created_at'] = pd.to_datetime(df['created_at'])
                df.set_index('created_at', inplace=True)
                
            elif data_type == "currency":
                data = result["data"]["exchange_rates"]
                if not data:
                    # Generate sample data for demo
                    data = self._generate_sample_currency_data(symbol, since, now)
                df = pd.DataFrame(data)
                df['created_at'] = pd.to_datetime(df['created_at'])
                df.set_index('created_at', inplace=True)
            
            return df
    
    def _generate_sample_weather_data(self, city: str, since: datetime, now: datetime) -> List[Dict]:
        """Generate sample weather data for demo purposes"""
        data = []
        current = since
        base_temp = 20 if city.lower() in ["new york", "london"] else 25
        
        while current < now:
            # Add some realistic variation
            temp_variation = np.sin(current.hour * np.pi / 12) * 5  # Daily temperature cycle
            temp = base_temp + temp_variation + np.random.normal(0, 2)
            
            data.append({
                "temperature": round(temp, 1),
                "humidity": round(50 + np.random.normal(0, 15)),
                "pressure": round(1013 + np.random.normal(0, 10)),
                "created_at": current.isoformat(),
            })
            current += timedelta(hours=1)
        
        return data
    
    def _generate_sample_crypto_data(self, symbol: str, since: datetime, now: datetime) -> List[Dict]:
        """Generate sample crypto data for demo purposes"""
        data = []
        current = since
        
        # Base prices for different cryptocurrencies
        base_prices = {"BTC": 45000, "ETH": 3000, "ADA": 0.5, "DOT": 7.5}
        base_price = base_prices.get(symbol.upper(), 1000)
        
        price = base_price
        while current < now:
            # Random walk with slight upward bias
            change = np.random.normal(0.0005, 0.02)  # 0.05% mean, 2% std
            price *= (1 + change)
            
            data.append({
                "price": round(price, 2),
                "market_cap": round(price * np.random.uniform(10000000, 1000000000)),
                "volume_24h": round(price * np.random.uniform(1000000, 100000000)),
                "change_24h": round(change * 100, 2),
                "created_at": current.isoformat(),
            })
            current += timedelta(minutes=15)
        
        return data
    
    def _generate_sample_currency_data(self, symbol: str, since: datetime, now: datetime) -> List[Dict]:
        """Generate sample currency data for demo purposes"""
        data = []
        current = since
        
        # Base rates for major currencies
        base_rates = {"EUR": 0.85, "GBP": 0.73, "JPY": 110.0}
        base_rate = base_rates.get(symbol.upper(), 1.0)
        
        rate = base_rate
        while current < now:
            # Small random changes
            change = np.random.normal(0, 0.001)  # 0.1% std deviation
            rate *= (1 + change)
            
            data.append({
                "rate": round(rate, 4),
                "created_at": current.isoformat(),
            })
            current += timedelta(minutes=30)
        
        return data
    
    async def analyze_trend(self, data_type: str, symbol: str, timeframe: str) -> Dict[str, Any]:
        """Analyze trend in time series data"""
        df = await self.get_time_series_data(data_type, symbol, timeframe)
        
        if df.empty:
            raise HTTPException(status_code=404, detail="No data found")
        
        # Use the primary value column based on data type
        if data_type == "weather":
            value_col = "temperature"
        elif data_type == "crypto":
            value_col = "price"
        elif data_type == "currency":
            value_col = "rate"
        else:
            value_col = df.select_dtypes(include=[np.number]).columns[0]
        
        values = df[value_col].values
        timestamps = np.arange(len(values)).reshape(-1, 1)
        
        # Linear regression for trend
        model = LinearRegression()
        model.fit(timestamps, values)
        
        trend_slope = model.coef_[0]
        r_squared = model.score(timestamps, values)
        
        # Calculate additional metrics
        current_value = values[-1]
        start_value = values[0]
        total_change = current_value - start_value
        percent_change = (total_change / start_value) * 100 if start_value != 0 else 0
        
        # Determine trend direction
        if abs(trend_slope) < 0.001:
            trend_direction = "sideways"
        elif trend_slope > 0:
            trend_direction = "upward"
        else:
            trend_direction = "downward"
        
        result = {
            "data_type": data_type,
            "symbol": symbol,
            "timeframe": timeframe,
            "trend_direction": trend_direction,
            "trend_slope": float(trend_slope),
            "r_squared": float(r_squared),
            "current_value": float(current_value),
            "start_value": float(start_value),
            "total_change": float(total_change),
            "percent_change": float(percent_change),
            "data_points": len(values),
            "analysis_timestamp": datetime.utcnow().isoformat(),
        }
        
        # Cache result
        if redis_client:
            cache_key = f"analysis:trend:{data_type}:{symbol}:{timeframe}"
            redis_client.setex(cache_key, 900, json.dumps(result))  # Cache for 15 minutes
        
        return result
    
    async def predict_values(self, data_type: str, symbol: str, days_ahead: int, model_type: str = "linear") -> Dict[str, Any]:
        """Predict future values using ML models"""
        df = await self.get_time_series_data(data_type, symbol, "30d")  # Use 30 days for training
        
        if df.empty:
            raise HTTPException(status_code=404, detail="No data found")
        
        # Select value column
        if data_type == "weather":
            value_col = "temperature"
        elif data_type == "crypto":
            value_col = "price"
        elif data_type == "currency":
            value_col = "rate"
        else:
            value_col = df.select_dtypes(include=[np.number]).columns[0]
        
        values = df[value_col].values
        timestamps = np.arange(len(values)).reshape(-1, 1)
        
        # Prepare future timestamps
        future_timestamps = np.arange(len(values), len(values) + days_ahead * 24).reshape(-1, 1)
        
        if model_type == "linear":
            model = LinearRegression()
        elif model_type == "polynomial":
            from sklearn.preprocessing import PolynomialFeatures
            poly_features = PolynomialFeatures(degree=2)
            timestamps = poly_features.fit_transform(timestamps)
            future_timestamps = poly_features.transform(future_timestamps)
            model = LinearRegression()
        else:  # moving_average
            # Simple moving average prediction
            window = min(24, len(values) // 4)  # Use last 24 hours or 1/4 of data
            recent_avg = np.mean(values[-window:])
            predictions = [recent_avg] * (days_ahead * 24)
            
            return {
                "data_type": data_type,
                "symbol": symbol,
                "model_type": model_type,
                "days_ahead": days_ahead,
                "predictions": predictions,
                "confidence_score": 0.5,  # Lower confidence for simple average
                "training_data_points": len(values),
                "prediction_timestamps": [(datetime.utcnow() + timedelta(hours=i)).isoformat() 
                                        for i in range(1, days_ahead * 24 + 1)],
                "analysis_timestamp": datetime.utcnow().isoformat(),
            }
        
        # Train model
        model.fit(timestamps, values)
        
        # Make predictions
        predictions = model.predict(future_timestamps)
        
        # Calculate confidence score (R-squared)
        confidence_score = model.score(timestamps, values)
        
        result = {
            "data_type": data_type,
            "symbol": symbol,
            "model_type": model_type,
            "days_ahead": days_ahead,
            "predictions": predictions.tolist(),
            "confidence_score": float(confidence_score),
            "training_data_points": len(values),
            "prediction_timestamps": [(datetime.utcnow() + timedelta(hours=i)).isoformat() 
                                    for i in range(1, len(predictions) + 1)],
            "analysis_timestamp": datetime.utcnow().isoformat(),
        }
        
        # Cache result
        if redis_client:
            cache_key = f"analysis:prediction:{data_type}:{symbol}:{days_ahead}:{model_type}"
            redis_client.setex(cache_key, 1800, json.dumps(result))  # Cache for 30 minutes
        
        return result
    
    async def detect_anomalies(self, data_type: str, symbol: str, sensitivity: float, window_size: int) -> Dict[str, Any]:
        """Detect anomalies in time series data"""
        df = await self.get_time_series_data(data_type, symbol, "7d")
        
        if df.empty:
            raise HTTPException(status_code=404, detail="No data found")
        
        # Select value column
        if data_type == "weather":
            value_col = "temperature"
        elif data_type == "crypto":
            value_col = "price"
        elif data_type == "currency":
            value_col = "rate"
        else:
            value_col = df.select_dtypes(include=[np.number]).columns[0]
        
        values = df[value_col].values
        
        if len(values) < window_size:
            raise HTTPException(status_code=400, detail="Not enough data for anomaly detection")
        
        # Calculate rolling mean and standard deviation
        rolling_mean = pd.Series(values).rolling(window=window_size).mean()
        rolling_std = pd.Series(values).rolling(window=window_size).std()
        
        # Detect anomalies using z-score method
        z_scores = np.abs((values - rolling_mean) / rolling_std)
        anomaly_threshold = sensitivity
        
        anomalies = []
        for i, (z_score, value, timestamp) in enumerate(zip(z_scores, values, df.index)):
            if z_score > anomaly_threshold:
                anomalies.append({
                    "index": i,
                    "timestamp": timestamp.isoformat(),
                    "value": float(value),
                    "z_score": float(z_score),
                    "expected_range": {
                        "lower": float(rolling_mean.iloc[i] - sensitivity * rolling_std.iloc[i]),
                        "upper": float(rolling_mean.iloc[i] + sensitivity * rolling_std.iloc[i])
                    }
                })
        
        result = {
            "data_type": data_type,
            "symbol": symbol,
            "sensitivity": sensitivity,
            "window_size": window_size,
            "total_data_points": len(values),
            "anomalies_detected": len(anomalies),
            "anomaly_rate": float(len(anomalies) / len(values)),
            "anomalies": anomalies,
            "analysis_timestamp": datetime.utcnow().isoformat(),
        }
        
        # Cache result
        if redis_client:
            cache_key = f"analysis:anomaly:{data_type}:{symbol}:{sensitivity}:{window_size}"
            redis_client.setex(cache_key, 600, json.dumps(result))  # Cache for 10 minutes
        
        return result

# Initialize analyzer
analyzer = DataAnalyzer()

# API endpoints
@app.get("/", tags=["Health"])
async def hello():
    return {
        "message": "Hello from Data Analyzer! ðŸ¤–",
        "service": "data-analyzer",
        "capabilities": ["trend_analysis", "prediction", "anomaly_detection", "correlation"],
        "timestamp": datetime.utcnow()
    }

@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health():
    return HealthResponse(
        status="ok",
        service="data-analyzer",
        timestamp=datetime.utcnow(),
        redis_connected=redis_client is not None,
        ml_models_loaded=True
    )

@app.post("/analyze/trend", tags=["Analysis"])
async def analyze_trend_endpoint(request: AnalysisRequest):
    """Analyze trend in time series data"""
    try:
        result = await analyzer.analyze_trend(
            request.data_type, 
            request.parameters.get("symbol", "BTC"), 
            request.timeframe
        )
        return {"success": True, "analysis": result}
    except Exception as e:
        logger.error(f"Trend analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze/predict", tags=["Analysis"])
async def predict_values_endpoint(request: PredictionRequest):
    """Predict future values using ML models"""
    try:
        result = await analyzer.predict_values(
            request.data_type,
            request.symbol,
            request.days_ahead,
            request.model_type
        )
        return {"success": True, "prediction": result}
    except Exception as e:
        logger.error(f"Prediction failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze/anomaly", tags=["Analysis"])
async def detect_anomalies_endpoint(request: AnomalyDetectionRequest):
    """Detect anomalies in time series data"""
    try:
        result = await analyzer.detect_anomalies(
            request.data_type,
            request.symbol,
            request.sensitivity,
            request.window_size
        )
        return {"success": True, "anomaly_detection": result}
    except Exception as e:
        logger.error(f"Anomaly detection failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/analyze/summary/{data_type}/{symbol}", tags=["Analysis"])
async def get_analysis_summary(data_type: str, symbol: str):
    """Get a comprehensive analysis summary"""
    try:
        # Run multiple analyses in parallel
        trend_task = analyzer.analyze_trend(data_type, symbol, "7d")
        prediction_task = analyzer.predict_values(data_type, symbol, 7, "linear")
        anomaly_task = analyzer.detect_anomalies(data_type, symbol, 2.0, 24)
        
        trend_result, prediction_result, anomaly_result = await asyncio.gather(
            trend_task, prediction_task, anomaly_task, return_exceptions=True
        )
        
        summary = {
            "data_type": data_type,
            "symbol": symbol,
            "timestamp": datetime.utcnow().isoformat(),
            "trend_analysis": trend_result if not isinstance(trend_result, Exception) else {"error": str(trend_result)},
            "prediction": prediction_result if not isinstance(prediction_result, Exception) else {"error": str(prediction_result)},
            "anomaly_detection": anomaly_result if not isinstance(anomaly_result, Exception) else {"error": str(anomaly_result)},
        }
        
        return {"success": True, "summary": summary}
    except Exception as e:
        logger.error(f"Summary analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    port = int(os.getenv("PORT", "3400"))
    print(f"ðŸ¤– Data Analyzer is running on port {port}")
    print(f"ðŸ“Š ML/AI Analysis capabilities: Trend, Prediction, Anomaly Detection")
    print(f"ðŸ”— Health check: http://localhost:{port}/health")
    print(f"ðŸ“– API docs: http://localhost:{port}/docs")
    
    uvicorn.run(
        "data-analyzer:app",
        host="0.0.0.0",
        port=port,
        reload=os.getenv("PYTHON_ENV") == "development"
    )