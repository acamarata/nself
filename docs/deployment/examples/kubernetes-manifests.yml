# Complete Kubernetes manifests for nself custom services
# Generated by: nself k8s convert
# These are production-ready examples with best practices

# ============================================================================
# NAMESPACE
# ============================================================================

apiVersion: v1
kind: Namespace
metadata:
  name: myapp
  labels:
    name: myapp
    environment: production

---
# ============================================================================
# CONFIGMAP - Shared configuration for all custom services
# ============================================================================

apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-services-config
  namespace: myapp
data:
  # Core services
  POSTGRES_HOST: "postgres"
  POSTGRES_PORT: "5432"
  POSTGRES_DB: "myapp_db"
  HASURA_GRAPHQL_ENDPOINT: "http://hasura:8080/v1/graphql"
  REDIS_HOST: "redis"
  REDIS_PORT: "6379"

  # Application config
  PROJECT_NAME: "myapp"
  BASE_DOMAIN: "example.com"
  NODE_ENV: "production"

---
# ============================================================================
# SECRETS
# ============================================================================

apiVersion: v1
kind: Secret
metadata:
  name: custom-services-secrets
  namespace: myapp
type: Opaque
stringData:
  # Database credentials
  POSTGRES_USER: postgres
  POSTGRES_PASSWORD: <generated-secure-password>

  # Hasura
  HASURA_ADMIN_SECRET: <generated-admin-secret>

  # Redis
  REDIS_PASSWORD: <generated-redis-password>

  # Custom service secrets
  STRIPE_SECRET_KEY: sk_live_...
  STRIPE_WEBHOOK_SECRET: whsec_...
  SENDGRID_API_KEY: SG....
  CLICKHOUSE_PASSWORD: <secure-password>
  AWS_SECRET_ACCESS_KEY: <aws-secret>

---
# ============================================================================
# CUSTOM SERVICE 1: Payment API
# ============================================================================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-api
  namespace: myapp
  labels:
    app: payment-api
    component: custom-service
    tier: backend
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # Zero-downtime updates
  selector:
    matchLabels:
      app: payment-api
  template:
    metadata:
      labels:
        app: payment-api
        component: custom-service
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8001"
        prometheus.io/path: "/metrics"
    spec:
      # Security
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        fsGroup: 1001

      # Service account
      serviceAccountName: custom-services-sa

      # Init container (wait for dependencies)
      initContainers:
      - name: wait-for-postgres
        image: busybox:1.35
        command: ['sh', '-c', 'until nc -z postgres 5432; do echo waiting for postgres; sleep 2; done']

      containers:
      - name: payment-api
        image: myregistry/payment-api:v1.0.0
        imagePullPolicy: IfNotPresent

        ports:
        - name: http
          containerPort: 8001
          protocol: TCP

        # Environment variables from ConfigMap
        envFrom:
        - configMapRef:
            name: custom-services-config
        - secretRef:
            name: custom-services-secrets

        # Service-specific environment variables
        env:
        - name: SERVICE_NAME
          value: "payment_api"
        - name: SERVICE_PORT
          value: "8001"
        - name: LOG_LEVEL
          value: "info"
        - name: MAX_CONNECTIONS
          value: "100"

        # Resource limits
        resources:
          requests:
            cpu: "250m"
            memory: "256Mi"
          limits:
            cpu: "1000m"
            memory: "512Mi"

        # Liveness probe - restart if unhealthy
        livenessProbe:
          httpGet:
            path: /health
            port: 8001
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3

        # Readiness probe - remove from load balancer if not ready
        readinessProbe:
          httpGet:
            path: /health
            port: 8001
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3

        # Startup probe - for slow-starting containers
        startupProbe:
          httpGet:
            path: /health
            port: 8001
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 30  # 150 seconds max startup time

        # Security context
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1001
          capabilities:
            drop:
            - ALL

        # Volume mounts (for writable directories)
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/.cache

      # Volumes
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}

      # Tolerations (for node taints)
      tolerations:
      - key: "custom-services"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"

      # Node affinity (prefer custom-services nodes)
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: workload-type
                operator: In
                values:
                - custom-services
        # Pod anti-affinity (spread across nodes)
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - payment-api
              topologyKey: kubernetes.io/hostname

---
# Service for Payment API
apiVersion: v1
kind: Service
metadata:
  name: payment-api
  namespace: myapp
  labels:
    app: payment-api
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8001"
spec:
  type: ClusterIP
  ports:
  - port: 8001
    targetPort: 8001
    protocol: TCP
    name: http
  selector:
    app: payment-api

---
# HorizontalPodAutoscaler for Payment API
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: payment-api-hpa
  namespace: myapp
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: payment-api
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # Scale up immediately
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 30
      selectPolicy: Max

---
# PodDisruptionBudget - ensure availability during maintenance
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: payment-api-pdb
  namespace: myapp
spec:
  minAvailable: 2  # Always keep at least 2 pods running
  selector:
    matchLabels:
      app: payment-api

---
# ============================================================================
# CUSTOM SERVICE 2: Notification Worker
# ============================================================================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: notification-worker
  namespace: myapp
  labels:
    app: notification-worker
    component: custom-service
    tier: worker
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: notification-worker
  template:
    metadata:
      labels:
        app: notification-worker
        component: custom-service
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001

      initContainers:
      - name: wait-for-redis
        image: busybox:1.35
        command: ['sh', '-c', 'until nc -z redis 6379; do echo waiting for redis; sleep 2; done']

      containers:
      - name: notification-worker
        image: myregistry/notification-worker:v1.0.0
        imagePullPolicy: IfNotPresent

        envFrom:
        - configMapRef:
            name: custom-services-config
        - secretRef:
            name: custom-services-secrets

        env:
        - name: SERVICE_NAME
          value: "notification_worker"
        - name: WORKER_CONCURRENCY
          value: "5"
        - name: MAX_RETRIES
          value: "3"

        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"

        # Workers don't expose HTTP, use exec probe
        livenessProbe:
          exec:
            command:
            - node
            - -e
            - "process.exit(0)"  # Custom health check script
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3

        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1001
          capabilities:
            drop:
            - ALL

        volumeMounts:
        - name: tmp
          mountPath: /tmp

      volumes:
      - name: tmp
        emptyDir: {}

---
# ============================================================================
# CUSTOM SERVICE 3: Analytics API
# ============================================================================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: analytics-api
  namespace: myapp
  labels:
    app: analytics-api
    component: custom-service
    tier: backend
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: analytics-api
  template:
    metadata:
      labels:
        app: analytics-api
        component: custom-service
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8003"
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001

      containers:
      - name: analytics-api
        image: myregistry/analytics-api:v1.0.0
        imagePullPolicy: IfNotPresent

        ports:
        - name: http
          containerPort: 8003

        envFrom:
        - configMapRef:
            name: custom-services-config
        - secretRef:
            name: custom-services-secrets

        env:
        - name: SERVICE_NAME
          value: "analytics_api"
        - name: SERVICE_PORT
          value: "8003"
        - name: BATCH_SIZE
          value: "1000"

        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "2000m"
            memory: "1Gi"

        livenessProbe:
          httpGet:
            path: /health
            port: 8003
          initialDelaySeconds: 30
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /health
            port: 8003
          initialDelaySeconds: 10
          periodSeconds: 5

        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1001

        volumeMounts:
        - name: tmp
          mountPath: /tmp

      volumes:
      - name: tmp
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: analytics-api
  namespace: myapp
spec:
  type: ClusterIP
  ports:
  - port: 8003
    targetPort: 8003
    name: http
  selector:
    app: analytics-api

---
# ============================================================================
# INGRESS - External access to custom services
# ============================================================================

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: custom-services-ingress
  namespace: myapp
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - payment-api.example.com
    - analytics-api.example.com
    secretName: custom-services-tls
  rules:
  - host: payment-api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: payment-api
            port:
              number: 8001
  - host: analytics-api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: analytics-api
            port:
              number: 8003

---
# ============================================================================
# SERVICE ACCOUNT & RBAC
# ============================================================================

apiVersion: v1
kind: ServiceAccount
metadata:
  name: custom-services-sa
  namespace: myapp

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: custom-services-role
  namespace: myapp
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: custom-services-rolebinding
  namespace: myapp
subjects:
- kind: ServiceAccount
  name: custom-services-sa
  namespace: myapp
roleRef:
  kind: Role
  name: custom-services-role
  apiGroup: rbac.authorization.k8s.io

---
# ============================================================================
# NETWORK POLICY - Restrict traffic to custom services
# ============================================================================

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: custom-services-netpol
  namespace: myapp
spec:
  podSelector:
    matchLabels:
      component: custom-service
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Allow from ingress controller
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8001
    - protocol: TCP
      port: 8003
  # Allow from other custom services
  - from:
    - podSelector:
        matchLabels:
          component: custom-service
  egress:
  # Allow to core services
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  - to:
    - podSelector:
        matchLabels:
          app: hasura
    ports:
    - protocol: TCP
      port: 8080
  # Allow DNS
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  # Allow external HTTPS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 80
